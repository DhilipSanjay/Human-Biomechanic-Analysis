{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Biomechanic Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhilipSanjay/Human-Biomechanic-Analysis/blob/main/Human_Biomechanic_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLoHWo07sD05"
      },
      "source": [
        "# Human Biomechanic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZnDFFmLr-mN"
      },
      "source": [
        "## Import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBDoHPZP-dvh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqfj27Wf9HoF",
        "outputId": "b9cb3746-ad0c-4926-b4e7-5be7d8e28526"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "PD_model = tf.keras.models.load_model(\"/content/drive/MyDrive/College Related/Project Work 1 - Sem 7/Saved Models/Final Models/PD_Final_Model.h5\")\n",
        "HY_model = tf.keras.models.load_model(\"/content/drive/MyDrive/College Related/Project Work 1 - Sem 7/Saved Models/Final Models/HY_Final_Model.h5\")\n",
        "FOG_model = tf.keras.models.load_model(\"/content/drive/MyDrive/College Related/Project Work 1 - Sem 7/Saved Models/Final Models/FOG_Final_Model.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tRVFQOawIx"
      },
      "source": [
        "## Physionet Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_SSCedlPkk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e927ed64-8526-44f2-a209-b3e4604ccae6"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "dataset_dir = \"/content/drive/MyDrive/College Related/Project Work 1 - Sem 7/Dataset/\"\n",
        "dataset_zip = dataset_dir + \"gait-in-parkinsons-disease-1.0.0.zip\"\n",
        "\n",
        "!unzip -q \"$dataset_zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvXBryVulPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577f826b-a347-4fe0-aeed-3d5c07e12d21"
      },
      "source": [
        "!mv gait-in-parkinsons-disease-1.0.0 Physionet\n",
        "len(os.listdir('Physionet'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N67tA4Ingi0Z"
      },
      "source": [
        "features = ['Time', 'L1' , 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', \n",
        "            'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', \n",
        "            'Total_Force_Left', 'Total_Force_Right']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAYQK53fsUqi"
      },
      "source": [
        "!mkdir CSV\n",
        "\n",
        "# reading given csv file \n",
        "# and creating dataframe\n",
        "for name in os.listdir('Physionet'):\n",
        "  if 'Co' in name or 'Pt' in name:\n",
        "    # print(name)\n",
        "    df = pd.read_csv('Physionet/' + name, header = None, sep='\\t')\n",
        "      \n",
        "    # adding column headings\n",
        "    df.columns = features\n",
        "      \n",
        "    # store dataframe into csv file\n",
        "    name = 'CSV/' + name.split('.')[0]+'.csv'\n",
        "    # print(name)\n",
        "    df.to_csv(name, index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwSvyujwzxS5",
        "outputId": "d687d5b0-f358-4ef1-85cc-0f93e043bbb9"
      },
      "source": [
        "demographics = pd.read_csv('Physionet/demographics.txt', delim_whitespace=True)\n",
        "sub_id = demographics.ID.to_list()\n",
        "sub_names = []\n",
        "for name in os.listdir('CSV'):\n",
        "  sub_name = name.split('_')[0]\n",
        "  sub_names.append(sub_name)\n",
        "\n",
        "print(\"Subjects Count (in demographics) = \", len(sub_id))\n",
        "print(\"Files count = \", len(sub_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subjects Count (in demographics) =  166\n",
            "Files count =  306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbRiGxHRQ_bd",
        "outputId": "1902c68c-b9a9-4ccb-edc2-5b971b9ed2a3"
      },
      "source": [
        "demographics.HoehnYahr\n",
        "demographics['HoehnYahr'] = demographics['HoehnYahr'].fillna(0)\n",
        "demographics.HoehnYahr.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    73\n",
              "2.0    55\n",
              "2.5    28\n",
              "3.0    10\n",
              "Name: HoehnYahr, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxMtwLPxUaAH",
        "outputId": "8da38dab-7b76-4ee6-f425-492f2f913b12"
      },
      "source": [
        "demographics['HoehnYahr'].replace({2.5: 1}, inplace=True)\n",
        "demographics['HoehnYahr'] = demographics['HoehnYahr'].astype(int)\n",
        "demographics.HoehnYahr.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    73\n",
              "2    55\n",
              "1    28\n",
              "3    10\n",
              "Name: HoehnYahr, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHlBADgVIHzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00a3007-a492-451d-dc18-600ddbeb502d"
      },
      "source": [
        "!mkdir Final\n",
        "!mkdir Final/train\n",
        "!mkdir Final/test\n",
        "\n",
        "count = len(os.listdir('CSV'))\n",
        "train_count = int(70/100*count)\n",
        "test_count = count - train_count\n",
        "\n",
        "print(\"Training Count Subjects = \", train_count)\n",
        "print(\"Test Count Subjects = \", test_count)\n",
        "\n",
        "category = 'train'\n",
        "counter = 1\n",
        "\n",
        "ypath = 'Final/y_' + category + '.txt'\n",
        "yfile = open(ypath, \"a\")\n",
        "hypath = 'Final/hyscore_' + category + '.txt'\n",
        "hyfile = open(hypath, \"a\")\n",
        "\n",
        "for name in os.listdir('CSV'):\n",
        "  if counter == train_count + 1:\n",
        "    yfile.flush()\n",
        "    yfile.close()\n",
        "    hyfile.flush()\n",
        "    hyfile.close()\n",
        "    \n",
        "    category = 'test'\n",
        "    ypath = 'Final/y_' + category + '.txt'\n",
        "    yfile = open(ypath, \"a\")\n",
        "    hypath = 'Final/hyscore_' + category + '.txt'\n",
        "    hyfile = open(hypath, \"a\")\n",
        "  print(counter, name, category)\n",
        "  \n",
        "  sub_name = name.split('_')[0]\n",
        "  sub_class = demographics[demographics['ID'] == sub_name]['Group'].to_string(index=False).strip()\n",
        "  hy_class = demographics[demographics['ID'] == sub_name]['HoehnYahr'].to_string(index=False).strip()\n",
        "  sub_data = pd.read_csv('CSV/' + name)\n",
        "  features = sub_data.columns.to_list()\n",
        "\n",
        "  full_size = 100\n",
        "  overlap = 0.5\n",
        "  overlap_size = int(full_size * overlap / 2)\n",
        "  entry_size = full_size - overlap_size\n",
        "  \n",
        "  for i in range(0, sub_data.shape[0], entry_size):\n",
        "    if sub_data.shape[0] >= i + entry_size + overlap_size:\n",
        "      yfile.write(sub_class + \"\\n\")\n",
        "      hyfile.write(hy_class + \"\\n\")\n",
        "      \n",
        "      for fname in features:\n",
        "        path_name = 'Final/' + category + '/'\n",
        "        file_name = fname + '_' + category + '.txt'\n",
        "        with open(path_name + file_name, 'a') as feat_file:\n",
        "          arr = sub_data.iloc[i:i + entry_size + overlap_size, ][fname].to_numpy()\n",
        "          # print(i, len(arr), end = ' | ')\n",
        "          feat_file.write(' '.join(map(str, arr)) + \"\\n\")\n",
        "\n",
        "  counter = counter + 1\n",
        "yfile.flush()\n",
        "yfile.close()\n",
        "hyfile.flush()\n",
        "yfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Count Subjects =  214\n",
            "Test Count Subjects =  92\n",
            "1 GaCo03_02.csv train\n",
            "2 GaCo16_02.csv train\n",
            "3 SiCo24_01.csv train\n",
            "4 JuPt01_05.csv train\n",
            "5 JuPt21_05.csv train\n",
            "6 JuPt23_07.csv train\n",
            "7 SiCo27_01.csv train\n",
            "8 SiPt19_01.csv train\n",
            "9 SiCo26_01.csv train\n",
            "10 GaPt25_02.csv train\n",
            "11 GaPt33_02.csv train\n",
            "12 GaCo12_01.csv train\n",
            "13 JuPt11_06.csv train\n",
            "14 GaCo22_01.csv train\n",
            "15 GaPt16_10.csv train\n",
            "16 JuPt23_02.csv train\n",
            "17 SiPt33_01.csv train\n",
            "18 SiCo17_01.csv train\n",
            "19 JuCo08_01.csv train\n",
            "20 JuCo12_01.csv train\n",
            "21 SiCo05_01.csv train\n",
            "22 JuPt29_02.csv train\n",
            "23 JuPt21_07.csv train\n",
            "24 JuPt15_06.csv train\n",
            "25 GaPt24_02.csv train\n",
            "26 GaCo06_01.csv train\n",
            "27 JuPt06_06.csv train\n",
            "28 JuPt11_07.csv train\n",
            "29 SiPt09_01.csv train\n",
            "30 SiPt37_01.csv train\n",
            "31 SiCo28_01.csv train\n",
            "32 JuPt06_07.csv train\n",
            "33 JuPt15_03.csv train\n",
            "34 SiCo08_01.csv train\n",
            "35 JuPt10_04.csv train\n",
            "36 JuPt09_05.csv train\n",
            "37 JuPt23_06.csv train\n",
            "38 SiCo15_01.csv train\n",
            "39 GaPt15_01.csv train\n",
            "40 GaPt26_01.csv train\n",
            "41 SiPt34_01.csv train\n",
            "42 SiPt27_01.csv train\n",
            "43 GaPt32_10.csv train\n",
            "44 SiCo16_01.csv train\n",
            "45 GaPt06_01.csv train\n",
            "46 JuPt26_03.csv train\n",
            "47 GaCo13_10.csv train\n",
            "48 JuPt03_06.csv train\n",
            "49 SiPt29_01.csv train\n",
            "50 JuPt28_03.csv train\n",
            "51 GaPt27_01.csv train\n",
            "52 JuCo24_01.csv train\n",
            "53 JuPt15_02.csv train\n",
            "54 JuPt11_05.csv train\n",
            "55 GaPt07_01.csv train\n",
            "56 JuPt09_04.csv train\n",
            "57 GaPt20_02.csv train\n",
            "58 JuPt28_04.csv train\n",
            "59 SiCo11_01.csv train\n",
            "60 SiPt38_01.csv train\n",
            "61 SiPt24_01.csv train\n",
            "62 GaPt21_01.csv train\n",
            "63 GaCo02_01.csv train\n",
            "64 GaCo14_02.csv train\n",
            "65 SiPt28_01.csv train\n",
            "66 SiPt30_01.csv train\n",
            "67 JuCo22_01.csv train\n",
            "68 JuPt11_04.csv train\n",
            "69 GaCo09_01.csv train\n",
            "70 JuPt10_06.csv train\n",
            "71 GaPt18_10.csv train\n",
            "72 JuPt06_03.csv train\n",
            "73 GaPt31_10.csv train\n",
            "74 SiPt17_01.csv train\n",
            "75 GaPt32_01.csv train\n",
            "76 SiCo18_01.csv train\n",
            "77 JuPt28_06.csv train\n",
            "78 GaPt24_10.csv train\n",
            "79 GaCo10_02.csv train\n",
            "80 JuPt15_01.csv train\n",
            "81 SiPt10_01.csv train\n",
            "82 GaPt27_10.csv train\n",
            "83 JuPt20_06.csv train\n",
            "84 SiPt36_01.csv train\n",
            "85 GaCo14_10.csv train\n",
            "86 GaPt14_10.csv train\n",
            "87 JuCo14_01.csv train\n",
            "88 JuPt18_01.csv train\n",
            "89 JuPt11_02.csv train\n",
            "90 SiCo23_01.csv train\n",
            "91 JuPt15_04.csv train\n",
            "92 GaPt15_10.csv train\n",
            "93 JuPt14_01.csv train\n",
            "94 GaPt33_01.csv train\n",
            "95 JuPt28_02.csv train\n",
            "96 GaPt08_01.csv train\n",
            "97 GaPt13_02.csv train\n",
            "98 JuPt10_02.csv train\n",
            "99 JuCo20_01.csv train\n",
            "100 JuPt06_05.csv train\n",
            "101 JuPt19_01.csv train\n",
            "102 GaPt16_02.csv train\n",
            "103 JuPt16_01.csv train\n",
            "104 GaCo13_01.csv train\n",
            "105 SiPt08_01.csv train\n",
            "106 JuPt15_07.csv train\n",
            "107 JuCo23_01.csv train\n",
            "108 SiCo13_01.csv train\n",
            "109 JuPt23_05.csv train\n",
            "110 GaPt21_02.csv train\n",
            "111 SiCo25_01.csv train\n",
            "112 JuCo18_01.csv train\n",
            "113 JuPt03_04.csv train\n",
            "114 JuPt21_03.csv train\n",
            "115 JuPt03_02.csv train\n",
            "116 GaPt09_02.csv train\n",
            "117 GaPt29_01.csv train\n",
            "118 JuPt03_03.csv train\n",
            "119 SiPt07_01.csv train\n",
            "120 JuPt10_07.csv train\n",
            "121 JuCo06_01.csv train\n",
            "122 SiCo09_01.csv train\n",
            "123 GaPt22_01.csv train\n",
            "124 SiPt40_01.csv train\n",
            "125 GaPt31_02.csv train\n",
            "126 GaPt30_01.csv train\n",
            "127 GaPt31_01.csv train\n",
            "128 JuPt08_01.csv train\n",
            "129 GaCo17_10.csv train\n",
            "130 GaPt18_02.csv train\n",
            "131 JuPt20_03.csv train\n",
            "132 JuPt26_07.csv train\n",
            "133 SiPt35_01.csv train\n",
            "134 SiPt22_01.csv train\n",
            "135 GaPt15_02.csv train\n",
            "136 JuPt12_01.csv train\n",
            "137 JuCo09_01.csv train\n",
            "138 JuPt15_05.csv train\n",
            "139 GaCo22_10.csv train\n",
            "140 GaPt17_02.csv train\n",
            "141 JuPt26_04.csv train\n",
            "142 GaCo09_02.csv train\n",
            "143 SiPt25_01.csv train\n",
            "144 GaCo02_02.csv train\n",
            "145 GaPt05_01.csv train\n",
            "146 GaCo04_02.csv train\n",
            "147 GaPt08_02.csv train\n",
            "148 JuPt20_01.csv train\n",
            "149 GaCo17_01.csv train\n",
            "150 JuPt04_01.csv train\n",
            "151 GaPt23_02.csv train\n",
            "152 SiPt15_01.csv train\n",
            "153 JuPt17_01.csv train\n",
            "154 JuPt27_01.csv train\n",
            "155 JuPt20_05.csv train\n",
            "156 JuPt03_07.csv train\n",
            "157 GaCo10_01.csv train\n",
            "158 GaPt29_10.csv train\n",
            "159 SiCo14_01.csv train\n",
            "160 SiCo07_01.csv train\n",
            "161 JuCo19_01.csv train\n",
            "162 GaCo15_01.csv train\n",
            "163 JuPt09_02.csv train\n",
            "164 GaCo05_01.csv train\n",
            "165 GaCo17_02.csv train\n",
            "166 JuPt26_06.csv train\n",
            "167 GaPt14_02.csv train\n",
            "168 SiPt16_01.csv train\n",
            "169 SiCo20_01.csv train\n",
            "170 JuPt20_07.csv train\n",
            "171 JuCo01_01.csv train\n",
            "172 JuPt29_01.csv train\n",
            "173 GaPt13_01.csv train\n",
            "174 JuCo02_01.csv train\n",
            "175 JuPt26_05.csv train\n",
            "176 GaPt28_02.csv train\n",
            "177 JuPt20_04.csv train\n",
            "178 JuCo05_01.csv train\n",
            "179 GaPt33_10.csv train\n",
            "180 SiPt05_01.csv train\n",
            "181 JuCo17_01.csv train\n",
            "182 GaPt12_02.csv train\n",
            "183 GaPt13_10.csv train\n",
            "184 JuCo13_01.csv train\n",
            "185 JuPt03_05.csv train\n",
            "186 GaPt26_10.csv train\n",
            "187 GaPt32_02.csv train\n",
            "188 JuPt21_06.csv train\n",
            "189 GaPt19_10.csv train\n",
            "190 JuPt24_02.csv train\n",
            "191 JuPt03_01.csv train\n",
            "192 GaPt07_02.csv train\n",
            "193 GaPt14_01.csv train\n",
            "194 GaPt27_02.csv train\n",
            "195 JuPt20_02.csv train\n",
            "196 SiPt04_01.csv train\n",
            "197 JuPt09_01.csv train\n",
            "198 JuCo16_01.csv train\n",
            "199 SiPt39_01.csv train\n",
            "200 JuPt09_03.csv train\n",
            "201 GaPt09_01.csv train\n",
            "202 SiPt02_01.csv train\n",
            "203 SiCo03_01.csv train\n",
            "204 GaPt04_01.csv train\n",
            "205 GaCo06_02.csv train\n",
            "206 GaCo07_01.csv train\n",
            "207 GaPt03_01.csv train\n",
            "208 GaPt30_10.csv train\n",
            "209 JuPt10_01.csv train\n",
            "210 SiCo12_01.csv train\n",
            "211 GaPt20_01.csv train\n",
            "212 JuPt29_05.csv train\n",
            "213 GaPt30_02.csv train\n",
            "214 JuPt01_02.csv train\n",
            "215 GaPt12_01.csv test\n",
            "216 GaPt17_01.csv test\n",
            "217 JuCo15_01.csv test\n",
            "218 GaCo14_01.csv test\n",
            "219 JuPt06_02.csv test\n",
            "220 JuPt10_03.csv test\n",
            "221 SiCo29_01.csv test\n",
            "222 JuPt29_07.csv test\n",
            "223 JuPt22_01.csv test\n",
            "224 SiPt18_01.csv test\n",
            "225 JuPt23_04.csv test\n",
            "226 SiCo21_01.csv test\n",
            "227 JuCo25_01.csv test\n",
            "228 SiCo01_01.csv test\n",
            "229 GaCo16_10.csv test\n",
            "230 GaPt16_01.csv test\n",
            "231 JuPt01_03.csv test\n",
            "232 GaPt18_01.csv test\n",
            "233 JuPt01_04.csv test\n",
            "234 SiCo22_01.csv test\n",
            "235 GaCo01_01.csv test\n",
            "236 JuPt23_03.csv test\n",
            "237 JuPt26_01.csv test\n",
            "238 JuPt23_01.csv test\n",
            "239 JuPt02_01.csv test\n",
            "240 GaPt29_02.csv test\n",
            "241 GaCo13_02.csv test\n",
            "242 JuPt29_04.csv test\n",
            "243 JuPt06_04.csv test\n",
            "244 SiPt14_01.csv test\n",
            "245 GaPt26_02.csv test\n",
            "246 GaCo15_02.csv test\n",
            "247 SiCo06_01.csv test\n",
            "248 GaCo03_01.csv test\n",
            "249 JuPt21_04.csv test\n",
            "250 JuPt21_02.csv test\n",
            "251 JuPt06_01.csv test\n",
            "252 GaPt19_01.csv test\n",
            "253 SiCo30_01.csv test\n",
            "254 JuPt05_01.csv test\n",
            "255 GaPt28_10.csv test\n",
            "256 SiPt13_01.csv test\n",
            "257 GaPt19_02.csv test\n",
            "258 JuCo03_01.csv test\n",
            "259 GaCo08_02.csv test\n",
            "260 JuPt11_01.csv test\n",
            "261 SiPt21_01.csv test\n",
            "262 GaPt28_01.csv test\n",
            "263 GaPt17_10.csv test\n",
            "264 JuCo07_01.csv test\n",
            "265 SiPt31_01.csv test\n",
            "266 GaPt22_02.csv test\n",
            "267 GaCo07_02.csv test\n",
            "268 SiCo10_01.csv test\n",
            "269 JuPt01_01.csv test\n",
            "270 GaCo11_01.csv test\n",
            "271 GaPt23_01.csv test\n",
            "272 GaCo04_01.csv test\n",
            "273 JuPt28_07.csv test\n",
            "274 JuPt28_05.csv test\n",
            "275 GaPt25_10.csv test\n",
            "276 SiPt32_01.csv test\n",
            "277 SiCo04_01.csv test\n",
            "278 JuPt10_05.csv test\n",
            "279 GaPt20_10.csv test\n",
            "280 JuPt29_06.csv test\n",
            "281 JuPt21_01.csv test\n",
            "282 GaCo08_01.csv test\n",
            "283 SiPt12_01.csv test\n",
            "284 GaPt24_01.csv test\n",
            "285 JuPt25_01.csv test\n",
            "286 SiPt20_01.csv test\n",
            "287 JuPt11_03.csv test\n",
            "288 JuCo04_01.csv test\n",
            "289 GaPt25_01.csv test\n",
            "290 GaPt23_10.csv test\n",
            "291 JuPt01_06.csv test\n",
            "292 JuPt28_01.csv test\n",
            "293 JuCo21_01.csv test\n",
            "294 JuPt07_01.csv test\n",
            "295 GaCo16_01.csv test\n",
            "296 SiCo19_01.csv test\n",
            "297 GaCo15_10.csv test\n",
            "298 JuCo26_01.csv test\n",
            "299 JuPt24_01.csv test\n",
            "300 SiPt23_01.csv test\n",
            "301 JuPt29_03.csv test\n",
            "302 GaPt22_10.csv test\n",
            "303 JuCo11_01.csv test\n",
            "304 JuPt13_01.csv test\n",
            "305 GaCo05_02.csv test\n",
            "306 GaPt21_10.csv test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unHHUiZ2ZMiS"
      },
      "source": [
        "def load_file(filepath):\n",
        "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files into a 3D array of [samples, timesteps, features]\n",
        "def load_category(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = np.dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "\n",
        "# load a dataset category, such as train or test\n",
        "def load_dataset_category(category, prefix=''):\n",
        "\t# load all 19 files as a single array\n",
        "\tfilenames = []\n",
        "\tfor fname in features:\n",
        "\t\tfilenames.append(category + '/' + fname + '_' + category + '.txt')\n",
        "  \n",
        "\t# load input data\n",
        "\tX = load_category(filenames, prefix)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + 'y_'+ category +'.txt')\n",
        "\thy = load_file(prefix + 'hyscore_'+ category +'.txt')\n",
        "\treturn X, y, hy\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy, trainHY = load_dataset_category('train', prefix)\n",
        "\tprint(trainX.shape, trainy.shape, trainHY.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy, testHY = load_dataset_category('test', prefix)\n",
        "\tprint(testX.shape, testy.shape, testHY.shape)\n",
        " \n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = tf.keras.utils.to_categorical(trainy)\n",
        "\ttrainHY = tf.keras.utils.to_categorical(trainHY)\n",
        "\ttesty = tf.keras.utils.to_categorical(testy)\n",
        "\ttestHY = tf.keras.utils.to_categorical(testHY)  \n",
        " \n",
        "\t# print(trainX.shape, trainy.shape, trainHY.shape, testX.shape, testy.shape, testHY.shape)\n",
        "\treturn trainX, trainy, trainHY, testX, testy, testHY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALGmMo3ZOEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342aaeb0-48de-43da-e5cc-c5253900eaa6"
      },
      "source": [
        "trainX, trainy, trainHY, testX, testy, testHY = load_dataset('Final/')\n",
        "print(\"Training Data = \", trainX.shape)\n",
        "print(\"Training Class = \", trainy.shape)\n",
        "print(\"Training HoehnYahr Class = \", trainHY.shape)\n",
        "print(\"Test Data = \", testX.shape)\n",
        "print(\"Test Class = \", testy.shape)\n",
        "print(\"Test HoehnYahr Class = \", testHY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30930, 100, 19) (30930, 1) (30930, 1)\n",
            "(13096, 100, 19) (13096, 1) (13096, 1)\n",
            "Training Data =  (30930, 100, 19)\n",
            "Training Class =  (30930, 2)\n",
            "Training HoehnYahr Class =  (30930, 4)\n",
            "Test Data =  (13096, 100, 19)\n",
            "Test Class =  (13096, 2)\n",
            "Test HoehnYahr Class =  (13096, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT9D5bIiarXW"
      },
      "source": [
        "## Daphnet Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n69XgeOMBga",
        "outputId": "4b39fe0d-6dd1-4575-fcec-7bdbaad7318b"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "fog_dataset_dir = \"/content/drive/MyDrive/College Related/Project Work 1 - Sem 7/Dataset/\"\n",
        "fog_dataset_zip = fog_dataset_dir + \"dataset_fog_release.zip\"\n",
        "\n",
        "!unzip -q \"$fog_dataset_zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlcLQeFwM52N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7352b1-3673-44df-edb4-ebdb4b83a6e2"
      },
      "source": [
        "class_var = 'annontations'\n",
        "features = ['Time', 'ankle-x', 'ankle-y', 'ankle-z', \n",
        "            'thigh-x', 'thigh-y', 'thigh-z',\n",
        "            'trunk-x', 'trunk-y', 'trunk-z', class_var]\n",
        "len(os.listdir('dataset_fog_release/dataset'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pthqxf0JiIBF"
      },
      "source": [
        "!mkdir FOG_CSV\n",
        "\n",
        "# reading given csv file \n",
        "# and creating dataframe\n",
        "for name in os.listdir('dataset_fog_release/dataset'):\n",
        "    fog_df = pd.read_csv('dataset_fog_release/dataset/' + name, header = None, sep=' ')\n",
        "      \n",
        "    # adding column headings\n",
        "    fog_df.columns = features\n",
        "    fog_df = fog_df[fog_df.annontations != 0]  \n",
        "\n",
        "    # store dataframe into csv file\n",
        "    name = 'FOG_CSV/' + name.split('.')[0]+'.csv'\n",
        "    # print(name)\n",
        "    fog_df.to_csv(name, index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72npFjr6mbWt",
        "outputId": "73c83846-0db4-4ff5-c8a8-e174de473831"
      },
      "source": [
        "!mkdir FOG_Final\n",
        "!mkdir FOG_Final/train\n",
        "!mkdir FOG_Final/test\n",
        "\n",
        "count = len(os.listdir('FOG_CSV'))\n",
        "\n",
        "train_count = int(85/100*count)\n",
        "test_count = count - train_count\n",
        "\n",
        "print(\"Training Count Subjects = \", train_count)\n",
        "print(\"Test Count Subjects = \", test_count)\n",
        "\n",
        "category = 'train'\n",
        "counter = 1\n",
        "\n",
        "ypath = 'FOG_Final/y_' + category + '.txt'\n",
        "yfile = open(ypath, \"a\")\n",
        "\n",
        "for name in sorted(os.listdir('FOG_CSV')):\n",
        "  if counter == train_count + 1:\n",
        "    yfile.flush()\n",
        "    yfile.close()\n",
        "    \n",
        "    category = 'test'    \n",
        "    ypath = 'FOG_Final/y_' + category + '.txt'\n",
        "    yfile = open(ypath, \"a\")\n",
        "  print(counter, name, category)\n",
        "  \n",
        "  sub_name = name.split('R')[0]\n",
        "  sub_data = pd.read_csv('FOG_CSV/' + name)\n",
        "  features = sub_data.columns.to_list()\n",
        "  \n",
        "  full_size = 128\n",
        "  overlap = 0.5\n",
        "  overlap_size = int(full_size * overlap / 2)\n",
        "  entry_size = full_size - overlap_size\n",
        "\n",
        "  for i in range(0, sub_data.shape[0], entry_size):\n",
        "    if sub_data.shape[0] >= i + entry_size + overlap_size:\n",
        "      sub_classes = sub_data.iloc[i:i+entry_size+overlap_size, ][class_var].to_list()\n",
        "      sub_class = max(sub_classes, key = sub_classes.count)\n",
        "      yfile.write(str(sub_class) + \"\\n\")\n",
        " \n",
        "      for fname in features:\n",
        "        if fname != 'Time' and fname != class_var:\n",
        "          path_name = 'FOG_Final/' + category + '/'\n",
        "          file_name = fname + '_' + category + '.txt'\n",
        "          with open(path_name + file_name, 'a') as feat_file:\n",
        "            arr = sub_data.iloc[i:i+entry_size+overlap_size, ][fname].to_numpy()\n",
        "            # print(i, len(arr), end = ' | ')\n",
        "            feat_file.write(' '.join(map(str, arr)) + \"\\n\")\n",
        "  counter = counter + 1\n",
        "yfile.flush()\n",
        "yfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Count Subjects =  14\n",
            "Test Count Subjects =  3\n",
            "1 S01R01.csv train\n",
            "2 S01R02.csv train\n",
            "3 S02R01.csv train\n",
            "4 S02R02.csv train\n",
            "5 S03R01.csv train\n",
            "6 S03R02.csv train\n",
            "7 S03R03.csv train\n",
            "8 S04R01.csv train\n",
            "9 S05R01.csv train\n",
            "10 S05R02.csv train\n",
            "11 S06R01.csv train\n",
            "12 S06R02.csv train\n",
            "13 S07R01.csv train\n",
            "14 S07R02.csv train\n",
            "15 S08R01.csv test\n",
            "16 S09R01.csv test\n",
            "17 S10R01.csv test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lrGh-Q1pv3"
      },
      "source": [
        "def load_file(filepath):\n",
        "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files into a 3D array of [samples, timesteps, features]\n",
        "def load_category(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = np.dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset category, such as train or test\n",
        "def load_dataset_category(category, prefix=''):\n",
        "\t# load all 10 files as a single array\n",
        "  filenames = []\n",
        "  for fname in features:\n",
        "    if fname != 'Time' and fname != class_var:\n",
        "      filenames.append(category + '/' + fname + '_' + category + '.txt')\n",
        "  \n",
        "  # load input data\n",
        "  X = load_category(filenames, prefix)\n",
        "  # load class output\n",
        "  y = load_file(prefix + 'y_'+ category +'.txt')\n",
        "  return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "  # load all train\n",
        "  trainX, trainy = load_dataset_category('train', prefix)\n",
        "  print(trainX.shape, trainy.shape)\n",
        "  # load all test\n",
        "  testX, testy = load_dataset_category('test', prefix)\n",
        "  print(testX.shape, testy.shape)\n",
        "\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "\n",
        "  neg, pos = np.bincount(trainy.reshape(1, trainy.shape[0])[0])\n",
        "  total = neg + pos\n",
        "  print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
        "  # one hot encode y\n",
        "  trainy = tf.keras.utils.to_categorical(trainy)\n",
        "  testy = tf.keras.utils.to_categorical(testy)\n",
        "\n",
        "  # print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy, neg, pos, total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTFlFGZG2ZcQ"
      },
      "source": [
        "fog_trainX, fog_trainy, fog_testX, fog_testy, neg, pos, total = load_dataset('FOG_Final/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMPRIAoFiH_n",
        "outputId": "1a289999-d70b-4a48-a901-2616b2c457cc"
      },
      "source": [
        "print(\"Training Data = \", fog_trainX.shape)\n",
        "print(\"Training Class = \", fog_trainy.shape)\n",
        "print(\"Test Data = \", fog_testX.shape)\n",
        "print(\"Test Class = \", fog_testy.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data =  (8713, 128, 9)\n",
            "Training Class =  (8713, 2)\n",
            "Test Data =  (3158, 128, 9)\n",
            "Test Class =  (3158, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8qYq-1Zd_zR"
      },
      "source": [
        "## Final Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA-OxLdJeBr9"
      },
      "source": [
        "def final_prediction(PD_model, HY_model, FOG_model, pd_data, pd_class, fog_data, fog_class):\n",
        "  # pd_data - 100 x 19\n",
        "  # fog_data - 128 x 9\n",
        "  pd_pred = np.argmax(PD_model.predict(np.expand_dims(pd_data, axis=0))) \n",
        "  hy_pred = np.argmax(HY_model.predict(np.expand_dims(pd_data, axis=0)))\n",
        "  fog_pred = np.argmax(FOG_model.predict(np.expand_dims(fog_data, axis=0)))\n",
        "\n",
        "  print(\"ORIGINAL CLASS\")\n",
        "  if np.argmax(pd_class) == 0:\n",
        "      print(\"Parkinson's Disease\")\n",
        "  else:\n",
        "    print(\"Control\")\n",
        "  \n",
        "  if np.argmax(fog_class) == 0:\n",
        "      print(\"No Freeze\")\n",
        "  else:\n",
        "    print(\"Freeze\")\n",
        "\n",
        "  print(\"-\"*20)\n",
        "\n",
        "  '''\n",
        "    PD - 0 \n",
        "    Control - 1\n",
        "  ------------\n",
        "    No Freeze - 0\n",
        "    Freeze - 1\n",
        "  '''\n",
        "  \n",
        "  print(\"PREDICTION\")\n",
        "  if pd_pred == 0 or fog_pred == 1:\n",
        "    print(\"Parkinson's Disease Detected\")\n",
        "    if fog_pred == 1:\n",
        "      print(\"Freezing of GAIT\")\n",
        "    else:\n",
        "      print(\"No Freezing of GAIT\")\n",
        "  else:\n",
        "    print(\"Healthy Control\")\n",
        "\n",
        "  \n",
        "  '''\n",
        "  Hoehn Yahr Scale\n",
        "    0.0\t- 0\n",
        "    2.5\t- 1\n",
        "    2.0\t- 2\n",
        "    3.0\t- 3\n",
        "  '''\n",
        "  print(\"Hoehn Yahr Scale Severity: \", end=\"\")\n",
        "  if hy_pred == 0:\n",
        "    print(\"Healthy Control\")\n",
        "  elif hy_pred == 1:\n",
        "    print(\"2.5 severity\")\n",
        "  elif hy_pred == 2:\n",
        "    print(\"2 severity\")\n",
        "  elif hy_pred == 3:\n",
        "    print(\"3 severity\")\n",
        "  \n",
        "  print(\"_\"*40, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtSWKieKlsF1",
        "outputId": "bd12e5a1-eb71-4bc2-f368-34039231b2be"
      },
      "source": [
        "for x in range(10):\n",
        "  i = random.randrange(0, min(len(testX), len(fog_testX)))\n",
        "  final_prediction(PD_model, HY_model, FOG_model, testX[i], testy[i], fog_testX[i], fog_testy[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "No Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2 severity\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2 severity\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Control\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Healthy Control\n",
            "Hoehn Yahr Scale Severity: Healthy Control\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Control\n",
            "Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: Healthy Control\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "No Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2.5 severity\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Control\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Healthy Control\n",
            "Hoehn Yahr Scale Severity: Healthy Control\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "No Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2 severity\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "No Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2 severity\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Control\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Healthy Control\n",
            "Hoehn Yahr Scale Severity: Healthy Control\n",
            "________________________________________ \n",
            "\n",
            "ORIGINAL CLASS\n",
            "Parkinson's Disease\n",
            "No Freeze\n",
            "--------------------\n",
            "PREDICTION\n",
            "Parkinson's Disease Detected\n",
            "No Freezing of GAIT\n",
            "Hoehn Yahr Scale Severity: 2 severity\n",
            "________________________________________ \n",
            "\n"
          ]
        }
      ]
    }
  ]
}